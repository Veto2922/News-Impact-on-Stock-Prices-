[build-system]
requires = ["flit_core >=3.2,<4"]
build-backend = "flit_core.buildapi"

[project]
name = "llms_evaluation"
version = "0.0.1"
description = "This project involves using a dataset from the LLM-EvaluationHub to evaluate and compare the effectiveness of different language models in detecting harmful prompts. Participants are expected to implement at least two models, perform data visualization, and analyze which model best identifies harmful content."
authors = [
  { name = "Abdelrahman Mohamed" },
]
license = { file = "LICENSE" }
readme = "README.md"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License"
]
requires-python = "~=3.12"

[tool.black]
line-length = 99
include = '\.pyi?$'
exclude = '''
/(
    \.git
  | \.venv
)/
'''

[tool.ruff.lint.isort]
known_first_party = ["llms_evaluation"]
force_sort_within_sections = true
