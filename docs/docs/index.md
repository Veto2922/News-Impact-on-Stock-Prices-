# LLMs Evaluation documentation!

## Description

This project involves using a dataset from the LLM-EvaluationHub to evaluate and compare the effectiveness of different language models in detecting harmful prompts. Participants are expected to implement at least two models, perform data visualization, and analyze which model best identifies harmful content.

## Commands

The Makefile contains the central entry points for common tasks related to this project.

